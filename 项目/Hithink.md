# chatbot 问答方案



## 项目背景

同花顺ifind终端（一般提供给B端的研究员用）在使用终端，希望有一个统一的对话入口，可以查询数据，表格，连接客服等，我们通过chatbot分流之后为用户提供问答场景的服务。

## 基于workflow的方案

**流程： 用户输入 -> 问句消歧（针对多轮问句） -> 模型规划  -> 反思 -> 正文生成**

#### 问句消岐

​		输入： 用户历史问句 + 历史回答 + 当前问题 -> 完整问句。

​        细节：用户问的问题可能和前面提问相关，会省略掉主体（之前说了公司名，后面用代词），时间等信息，或者针对上一轮答案，继续追问（例如，上一次用户提问，xxx公司买入点位是多少？第二次再提问：你这个xx毛利润为什么这么低，行业平均是多少？再和市场其它公司对比一下）。对于这种有指代关系的问句，需要先做一次改写，补充相关的背景信息后得到完整问句（xxx公司的毛利率是xxx，行业平均是多少？再和市场其它公司对比一下），之后再去走工作流。这种改写的方式相当于把多轮对话做成了单论对话，优势是不需要在后续的workflow都带上历史对话信息，此时改写后的问句已经是完整的了，更加简单。

​	**问1：**为什么不考虑澄清？

​    **答1：**看到回答完全不对，用户提前终止，重新提问。如果再加一个澄清的链路，模型有容易过度澄清的倾向（一定要上下文很全才肯回答）

-   历史上下文处理


​       尝试多种策略：历史问句全保留，尝试直接对所有答案进行压缩，如果用户提问和之前答案的某个点相关，压缩后丢失这部分信息会导致改写不准确。后续改成只保留最近2轮完整问答（改写后的），前面答案摘要成多个bullet points。

#### 模型规划

模型规划的部分其实就是做问句拆解，将原始问句转为适配取数工具和搜索工具的子问题。

- 之前的问题：用户觉得问句回答的太简单了。例如，用户提问公司盈利能力怎么样，只取归母净利润，毛利率几个指标分析比较片面。用户想要更全面的分析，例如盈利方法，可以往细了说，按每个业务划分。
- 改进：对于取数工具来说，只用一个prompt分解的子问句不够全面，使用LLM进行联想（指标不够专业，并且无法和库里面完全匹配上） -> embedding匹配（可以召回比较多的指标，有一定联想能力，但是精排不太行） -> **分析框架使用**（研究员针对财报，公司，行业，基金，债券，政策等10类主题构建了分析方法库），分析框架太大了，只取到三级。根据这个分析框架用大模型生成query。 对于搜索工具来说，分解给搜索问句的子问句有自己的要求，例如如果是时序问句，2020-2023年xxx，最好可以分解成2020，2021,...,2023等多个子问句。此外，定位也不太一样，取数主要是找指标，重点在数据。搜索主要是找网上的消息面，财报解读，分析师评级等较为笼统的东西。

#### 反思

​	模型判断有无信息缺失： xxx工具没找到，换一个工具/子问句再搜一遍。输入是之前调用工具的subquery + 取数结果。输出是每个工具需要补充的信息（同一个工具不能再找完全一样的subquery），再收集一轮结束规划。我们发现增加一轮反思之后可以在一定程度上提升系统解决多跳问题的能力。

#### 正文生成

a. 把搜集到的信息，编号之后放在prompt里面，生成带有引用的答案。

b. 模型生成表格需要引用提供的素材，后台拿到引用之后，对应到完整的ifind终端的数据库。

c.  LLM判断问句是否有可视化意图，如果有可视化意图，把指标和问句传入画图工具，生图后插入文章末尾。

#### **数据合成**

1. 用户query合成

   为了模型sft的效果，query要多样，需要选择不同类型的query合成数据。冷启动使用产品提供的query，然后进行意图分解，把原始query+意图（动态生成150+）作为seed，生成多样性更丰富的query。(后续加上回流的query，jaccard_similarity去重)。之后让大模型判断难度，分为简单（查指标，新闻），中等（ 财报解读），困难（荐股，预测），大概按照60%，30%，10%的比例筛选3000条，用于后续workflow合成数据。我们发现用户有时候会输出有问题的问句，因此我们也尝试了故意改了一些有错别字，标点符号错误的query。

2. 问句消歧

   输入是： 问1+答1+ ... + 问n，输出是对当前用户提出的问n的改写。

   多轮query合成考虑几个情况：1. 问n和之前问句有关，答案毫无关联  2. 问n和之前答案有关 3. 还需要考虑间隔是多少。

   每种情况都要合成一点数据。第一种情况很好解决，直接拿线上的多轮问句，然后合成问n的时候考虑和之前哪个问句相关。第2种情况，线上真实数据比较少，因此需要让模型对某一个答案进行追问。

   400条SFT数据。qwen-72B合成  + gpt筛选、人工校对（qwen72改写后，先让gpt判断改写是否正确和判断理由，确实有歧义的让人校对）。后续改成deepseek做消歧，效果提升明显。（基本无需人工干预）

   

   问1：为什么不做DPO？

   答1：任务比较简单，在SFT的情况下改写准确率能到96%以上，再训下去没有太大意义。

3. 规划子问题分解

​	1000条SFT数据+300DPO。输入是改写后的问句+分析框架，输出是json格式，包括工具名，工具的参数subquery。人工（看query是否合理，手动修改一部分）+大模型合成的方式。

​	 亮点：分析框架比较复杂，可能会导致过度规划，比如有拓扑顺序的需要在第二轮调用工具，但是在第一次规划就调用了（例如，xx公司的和其竞争对手相比，业绩表现如何。模型可能在第一次规划，直接取数 xxx公司竞争对手盈利能力，但是应该是要先找到竞争对手有哪些，之后再取数）这种情况，DPO是有效的。另外，线上数据回流对于json格式出错（```json）也有几十条DPO数据。我们训练用了分析框架，但是推理的时候没用，这样可以快很多。

4. 反思

   100DPO。上下文太长，很难校对素材哪些信息没有，生成query之后利用embedding模型反向匹配素材，模型判断是否真的还缺少信息。如果素材已有，让模型重新生成一个query。

5. 正文生成

​		输入是prompt+ 之前找到的取数表格 + 搜索资料，输出是纯文本正文。	

​		1000SFT+ 100DPO。我们会用多个模型，包括qwen72b（后面换成qwen-2.5 72b），R1生成多份答案。之后根据模型的pair-wise评分选择分数最好的，分成几个维度：准确性，逻辑性，专业性，引用正确性。然后过滤掉一些答案比较短的数据，控制答案在1000-1500字符左右（太长容易幻觉）。

​	亮点：  

a. 幻觉检测。R1生成的答案比较多幻觉，我们用gpt-4o做了两步策略修复答案正文数据。第一，先让模型输出哪里出现了时间错误，数值错误，素材和答案事件不匹配；第二，把这些信息当作第二轮对话，让模型改正。这么做只能解决65%左右的问题。（因为有些是模糊的，不好判定是幻觉）

b.线上回流做DPO。对于回答的一些格式错误，例如溯源错误，答案太短的，会让模型重新生成一遍答案，人工校对之后做DPO。

#### 效果评价

LLM-as-judger方法，我们只评价最后结果，尝试直接对答案打分，分了几个维度，但是和人评价一致性太低。尝试过SPCT方式构造动态的打分prompt，效果也不好。

最终和研究员一起构建了33条的小数据集，包括问题和答案。问题选取事件解读，财务分析类的，行业分析，可以人工写答案（从研报中精简）。把多个版本的workflow/模型，标准答案放在一起，做pair-wise打分，这样比较准确。

产品评分，人工抽取答案和竞品对比。我们也会把竞品和自己模型的放在一起，打分并且生成理由。分为几个等级（差，较差，平，好，较好），如果人工和自动评价差了两个等级，会让人重新看一下模型说的有没有道理。

#### **经验&教训**

1. 模型幻觉问题

   广义上幻觉就是模型回答错误，胡说八道，狭义的幻觉是指模型本身有这方面的知识，但是经过alignment处理之后就回答不对了。目前来说幻觉属于无法根除的问题，通常都是从数据层面缓解幻觉现象。

2. 指令跟随能力

   通常是预训练获得的能力，考虑sft阶段会削弱。有以下方式可以尽可能不伤害模型指令跟随能力： 1. 控制上下文长度。2.数据多样性，配比。（进化指令，添加约束，增加例子，深度，广度提升）  3.模型回答之前再输出一遍问题（MATHIF的做法）

## 类o3，agentic的deep research方案

两个需求驱动改进：1.现有workflow流程比较死板。对于有些简单问题规划太复杂，而对于困难问题只能思考一次+反思，深度和广度都不够，无法处理复杂多跳问题。2. 产品想要增加宽基生成策略的功能，现有workflow框架没有代码生成和执行无法解决。例如：根据过去三个月的市场表现，如果把恒生互联网科技三巨头公司进行投资组合的买入/卖出决策主要由双重移动平均线策略决定，那么该策略的最佳实施将产生多少回报率？

本方案重点解决deep research核心两个问题:  （1）规划短思考 （2）记忆管理

**项目流程：规划收集信息 -> 回答。**

亮点：类o3的短思考+工具调用。引入MCP标准化工具调用方式，加上记忆管理方法，可以实现15+轮次的思考+工具调用。引入代码工具，可以对数据进行计算，以及实现**图文混排**。

#### 1. 规划思考

i. **标准化工具调用**

- 之前的问题：工具描述，参数，任务描述都放在一个prompt里面，管理混乱，无法适配现在MCP接入方式。
- 改进：所有工具标准化openai的调用格式，传入tool里面，后续参考qwen-agent统一为mcp调度。为了做到完全的工具调用，把think（思考也可以拆分论据、观点攻击；数据洞察；逻辑归因等），finish也做成了工具，如果是简单问题会直接调用finish工具。；将搜索引擎工具拆分为search，fetch（后续说明原因）。

ii.  **高效规划**： 

- 之前的问题： 大多数推理模型，deepseek，qwq等思考过程太长，一个简单的查指标问题也要分析一大段，而且中途调用工具还会反复犹豫，耗时太长且给用户展示的效果很差。

- 改进方式：使用最强的推理模型o3生成短思考+工具调用的REACT过程。但是o3有反扒机制，不会输出中间的思考过程，即使在加了think工具后，模型不能保证每次都稳定调用，这样模型一直在调用工具，但是用户不知道模型的推理过程。我们的解决方法是，在每个工具都额外添加一个参数，这个参数描述是模型想调用工具这个工具干什么。

​	问1：还有别的规划方式吗？

​	答1：从RAG -> GraphRAG -> agentic RAG。我们用的是叫做agentic RAG范式，就是说和普通RAG不一样。传统RAG更多关注在怎么搜索，比如前面提到的数据库建设，粗召精排和query改写等。而我们的agentic RAG不是一次性做检索，然后回答，规划和搜集信息是交错进行的，解决的任务也从简单单跳变成复杂多跳。另外还有GraphRAG，能处理一些普通RAG无法解决的全局问题，但是成本高。其它可以参考skywork，生成todo list，之后再一步一步执行。



iii. **python解释器**

- 之前的问题：取数工具是一个综合的接口，公司做的不是很好，一个问句会出来很多相关，不相关的表格（例子，xxxx），我们用字符串填充到tool call的message里面表格又多又长，需要做截断。但是取的数据（尤其是一个时间序列的）被截断之后，模型无法对结构化的数据做分析计算（举个例子，~~xx公司股价最高的三天有规律吗，~~），并且生成答案的时候，如果要画图不能画时间序列太长的图。（xx公司近5年的成长曲线是什么样的，用折线图展示公盈利能力，股票价格变化趋势）

- 改进方式：引入python代码解释器工具。1. 把取出来的数据全部转为dataframe对象，按照 sheet_1 :  df.head() + df.tail()  + df.info() 这种格式组织dataframe，模型可以看到这个dataframe前面几行，后面几行是什么，另外也知道各个shema是什么格式（字符串，float），每个shema有多少数据为空。同时在prompt中告诉模型，可以直接引用变量进行操作。2. 用ipython内核执行代码，工具返回的是stdout，traceback等关键信息。 3. 采用函数式编程方式，让模型每次生成单个功能的函数，然后打印函数执行的结果。这样可以避免模型生成一大串代码，但是代码中间其实已经报错了。（dataframe经常有空值，另外dataframe日期格式不统一，2025/7/27，20250207，2025-02-07，需要模型写代码先转换）

  问1：为什么不用codeact？

​		答1： codeact是把工具调用也做成了code block的形式，这样做的好处在于模型可以直接引用自己调用工具后的变量，缺点是无法让模型使用自己的对象/变量，以及每次调用一次函数都必须print保证不会出错。而我们的方法是自己手动注入，实现复杂一些，但是更加灵活。（提前告知模型对象结构是什么，以及可以使用全局变量）

iv.**分层记忆管理**

亮点：三级记忆压缩。

- tool级别信息压缩。取数工具输出太长改成dataframe，显著压缩效率。搜索素材太长，把搜索工具拆分为search和fetch。search工具得到结果之后，只会返回的是 id、时间、标题、摘要。如果模型要看全文，需要使用fetch工具，传入文章id，之后可以拿到全文。对于一些无用的网页信息，模型通过标题+摘要就能筛选掉，而如果是干货比较多的，可以进一步查看细节。

- step级别信息压缩。之前的问题：搜索接口不稳定/代码执行出错，模型反复调用工具，规划思考轮数太多，导致上下文超长。（例如，模型多次修改代码得到可以正常运行的代码之后，模型在下一轮规划其实没必要带上之前的上下文，只需要保留正确的代码就行）。step级别压缩是指，如果到达70%的上下文阈值，会对中间的工具调用过程进行删除。（还可以参考openhands，不仅仅是删除，还会进行摘要。）现在还在继续验证这个方案，主要是中间有些过程不太好判断是否可以删掉。

- conversation级别压缩。之前的问题：workflow额外引入了一个问句改写功能。改进：直接把用户的历史问句+答案保存到message里面，只保留最近两轮的完整对话记录（last-2 conversation），再之前的会对答案进行摘要，（问句比较短，仍然保留），最多支持10轮对话。

  

  问1：为什么tool级别不用rag，或者embedding召回方法？

  答1：尝试过embedding方法对网页来说效果不好，有几个重大缺陷。a. 网页如果太长，需要chunk然后和搜索query做embedding匹配加rerank，效率太低，对于问答系统来说无法承受。b. 有些抽象的概念很难和正文匹配。（例如，xx公司现在值得买吗？如果把这个和正文匹配，很容易得到一些所谓自媒体发布的推广。但其实，如果保留正文中的一些新闻消息，或者公司的财务数据，模型可以自己生成观点分析，而这一类的chunk对于embedding来说很难匹配上）
  
  ​	
  
  问2：和memory os论文的记忆管理有什么区别？
  
  答2：memory os重点是conversation级别的压缩，分为了短期记忆，中期记忆，长期记忆。它参考的是计算机多级缓存的思想，把一轮对话看作是一个page页面，进行资源调度，非常巧妙。相比我们的方案的优势在于，第一，它把所有对话组织成链条，更加清晰知道哪些对话之间是有关联的（我们的conversation压缩是保留原始对话，不做任何处理），并且用中期记忆去检索这个segment；第二，他的长期记忆做了用户画像和agent画像，这个是我们系统没有考虑到的。也就是说，他在做segment更新到长期记忆的时候，对segment处理成了关于用户的事实（短句子），以及agent是怎么回答用户的。增加agent画像的好处的是保持回答风格的统一，例如用户之前问荐股问题，推荐了xxx，再问哪些股票不看好，你就不能说xxx不好这种自相矛盾的观点。
  
  

#### 2.答案生成	

直接把搜索到的dataframe，搜索结果，python执行记录拼在一个prompt。模型按markdown格式生成正文。如果需要画图，采用这种格式<figure> ```python 画图代码块````</figure>。画图工具采用ploty，可以生成交互的图，生成完之后执行代码运行。表格直接引用，dataframe，后台解析到替换成表格组件。

#### 3.数据合成

进行中...

#### 4.GRPO训练

针对代码解释器做GRPO。奖励包括格式奖励：函数式编程风格，函数长度（防止被hacking，模型只看一个dataframe），执行正确。

现象：1.编程风格 reward稳定上升。 2. 函数长度reward先比较陡峭地上升，然后平滑波动   3. 只训一轮，否则容易被hacking，模型生成的总体代码太短。

#### 5.其它

分析了多轮对话分析模式失败的原因。（2505.06120v1）

（1）过早尝试回答：在一开始信息不明确，就规划好如何行动，会导致性能显著下降。早期的假设会随着后面收集信息冲突，使得整个react脱轨。解决方法：尽量不要做太全的规划，并且用推理能力比较强的模型，可以发现不合理的地方并且修改计划。或者引入一个可变的计划列表（jina，skywork等）

（2）中间信息遗忘。具体表现为模型反复做过之前执行的行动，或者重复生成。解决方法是尽可能保持简单上下文，16-24k是比较好的。

## 端到端强化学习

问1：**为什么没有用端到端RL？**

答1：客观上来说，公司接口不太支持，训练集群无法调用搜索，取数接口，尤其是取数还有并发限制。

问2：如果要用端到端RL做deepresearch，要怎么设计整个训练流程，数据合成怎么做？

答2：我们的方法工具比较复杂，不仅有搜索，还有取数，写代码。一个思路是分开做强化学习，例如设计一些情景搜索（类似openai例子），代码数据推理，数据分析任务（表格查询）去做，这样answer是明确的，先让模型学会哪个工具在什么场景适合，同时可以参考kimi引入一定的过程奖励。

问3：怎么做短思考？

答3：数据层面，可以把thinking压缩之后再训练模型。模型层面，可以用非推理模型，模拟推理模型。算法层面，强化学习，控制长度reward。

## 其它

#### prompt怎么写？

有以下几个注意点：

- 格式化输出，最好用json或者xml。模型输出这个结构化数据比较容易，也好解析。如果格式出错可以用正则匹配。例如，模型生成工具列表： 1. report: xx 公司研报 2.search：xx新闻。这种多了需要解析的时候考虑把无关的去除。
- few-shot。只在关键的地方写，比如在智能query生成的时候，可以给几个例子帮助模型理解。举例子的时候最好格式多样一些！而不是统一的模板，例如模型的thought部分太简单，可以写几个good例子，告诉模型对于复杂问题，可以先仔细分析需要哪些信息，再做规划，而不是直接把问题扔到搜索工具中。再写一个badcase，告诉模型如果是简单的（查询今天天气，不要写一大串思考内容）。另外，格式之类的（1，（一））这种可以混用，没必要非要统一。另外生成答案的prompt，可以多准备几套，混合着用。
- 注意利用kv-cache。尽量把system做成固定的，可以变的部分，例如时间，用户问题放在后面。这样对不同的问题，可以复用kv-cache。
- 多次迭代。研究员+算法 初步敲定prompt（规划，工具描述）-> 大模型调优 -> 线上发现问题再放到prompt

#### 后续改进

短期计划：1.  计划做端到端的RL训练，尝试类似kimi-researcher那种方式对整个轨迹都做强化学习。或者类似websa

- 转为multi-agent方案。难点是agent之间的通信，可以类似qwen-agent分配任务，或者openhands基于时间驱动，消息订阅。最好多看看代码生成这一块，生成代码是一个比较复杂的任务，需要有很强的记忆管理：  1. 要读取代码目录，输入不会短 2. 要生成代码，cursor一次性创建多个文件，输出也长 3. 用户经常会有很复杂的需求，需要交轮次多。

- 尝试GRPO应用到答案生成，难点是评价答案的好坏，可以参考BRPO。（SPCT评估模板，对答案进行pair-wise打分，reference随机选择一个group的几个数据）

- 多轮对话，需要优化dataframe保存的逻辑，直接注入全部的变量太多了。

- 业务改进，可以和其它功能联动，例如操作终端界面，生成更好的量化策略。
