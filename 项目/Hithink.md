# chatbot 问答方案



## 项目背景

同花顺ifind终端（一般提供给B端的研究员用）在使用终端，希望有一个统一的对话入口，可以查询数据，表格，连接客服等，我们通过chatbot整合了这些功能。

## 基于workflow的方案

**流程： 用户输入 -> 问句消歧（针对多轮问句） -> 模型规划  -> 反思 -> 正文生成**

1. 问句消岐

​		输入： 用户历史问句 + 历史回答 + 当前问题 -> 完整问句。

​        细节：用户问的问题可能和前面提问相关，会省略掉主体（之前说了公司名，后面用代词），时间等信息，或者针对上一轮答案，继续追问（例如，上一次用户提问，xxx公司买入点位是多少？第二次再提问：你这个xx毛利润为什么这么低，行业平均是多少？再和市场其它公司对比一下）。对于这种有指代关系的问句，需要先做一次改写，补充相关的背景信息后得到完整问句（xxx公司的毛利率是xxx，行业平均是多少？再和市场其它公司对比一下），之后再去走工作流。

​     	数据合成：qwen-72B合成  + gpt筛选、人工校对（qwen72改写后，先让模型判断改写是否正确和判断理由，确实有歧义的让人校对）。后续改成deepseek做消歧，效果提升明显。（基本无需人工干预）

​	**问1：**为什么不考虑澄清？

​    **答1：**看到回答完全不对，用户提前终止，重新提问。如果再加一个澄清的链路，模型有容易过度澄清的倾向（一定要上下文很全才肯回答）

-   历史上下文处理


​       尝试多种策略：历史问句全保留，尝试直接对所有答案进行压缩，如果用户提问和之前答案的某个点相关，压缩后丢失这部分信息会导致改写不准确。后续改成只保留最近3轮完整问答（改写后的），前面答案摘要成多个bullet points。

2. 模型规划

   模型规划的部分其实就是做问句拆解，将原始问句转为适配取数工具和搜索工具的子问题。

   - 之前的问题：用户觉得问句回答的太简单了。例如，用户提问公司盈利能力怎么样，只取归母净利润，毛利率几个指标分析比较片面。用户想要更全面的分析，例如盈利方法，可以往细了说，按每个业务划分。
   - 改进：对于取数工具来说，只用一个prompt分解的子问句不够全面，使用LLM进行联想多个指标 -> 分析框架使用（研究员针对财报，公司，行业，基金，债券，政策等10类主题构建了分析方法库），分析框架太大了，只取到二级。根据这个分析框架生成指标，再和前面的LLM联想的去重。 对于搜索工具来说，分解给搜索问句的子问句有自己的要求，例如如果是时序问句，2020-2023年xxx，最好可以分解成2020，2021,...,2023等多个子问句。此外，定位也不太一样，取数主要是找指标，重点在数据。搜索主要是找网上的消息面，财报解读，分析师评级等较为笼统的东西。

3. 反思

​	模型判断有无信息缺失： xxx工具没找到，换一个工具/子问句再搜一遍。输入是之前调用工具的subquery + 取数结果。输出是每个工具需要补充的信息（同一个工具不能再找完全一样的subquery），再收集一轮结束规划。我们发现增加一轮反思之后可以在一定程度上提升系统解决多跳问题的能力。

4. 正文生成

   a. 把搜集到的信息，编号之后放在prompt里面，生成带有引用的答案。

   b. 模型生成表格需要引用提供的素材，后台拿到引用之后，对应到完整的ifind终端的数据库。

   c.  LLM判断问句是否有可视化意图，如果有可视化意图，把指标和问句传入画图工具，生图后插入文章末尾。

**数据合成**

1. 用户query合成

   为了模型sft的效果，query要多样，需要选择不同类型的query合成数据。从两个维度考虑：意图，难度。

2. 规划子问题分解

3. 正文生成

**效果评价**

LLM-as-judger方法，我们只评价最后结果，尝试直接对答案打分，分了几个维度，但是和人评价一致性太低。

自己构建了30条的小数据集，包括问题和答案。把多个版本的workflow/模型，标准答案放在一起，做pair-wise打分，这样比较准确。

产品评分，人工抽取答案和竞品对比。我们也会把竞品和自己模型的放在一起，打分并且生成理由。分为几个等级（差，较差，平，好，较好），如果人工和自动评价差了两个等级，会让人重新看一下模型说的有没有道理。最后workflow效果显著优于妙想（东方财富），4月份的版本和wind持平。



# 类o3，基于REACT的deep research方案

两个需求驱动改进：1.现有workflow流程比较死板。对于有些简单问题规划太复杂，而对于困难问题只能思考一次+反思，深度和广度都不够，无法处理复杂多跳问题。2. 产品想要增加宽基生成策略的功能，现有workflow框架没有代码生成和执行无法解决。例如：将A股市值前五的车企公司和美股市值前10的车企，做成一个ETF，假设分别以4：6的比例投资，每个市场的公司均等持仓。我从去年7月份一次性投入10w，现在收益率是多少，面临多大回撤？

本方案重点解决deep research核心两个问题:  （1）规划短思考 （2）记忆管理

**项目流程：规划收集信息 -> 回答。**

亮点：类o3的短思考+工具调用。引入MCP标准化工具调用方式，加上记忆管理方法，可以实现15+轮次的思考+工具调用。引入代码工具，可以对数据进行计算，以及实现**图文混排**。

#### 1. 规划思考

i. **标准化工具调用**

- 之前的问题：工具描述，参数，任务描述都放在一个prompt里面，管理混乱，无法适配现在MCP接入方式。
- 改进：所有工具标准化openai的调用格式，传入tool里面，后续参考qwen-agent统一为mcp调度。为了做到完全的工具调用，把think，finish也做成了工具，如果是简单问题（）会直接调用finish工具。，代码工具；将搜索引擎工具拆分为search，fetch（后续说明原因）。

ii.  **高效规划**： 

- 之前的问题： 大多数推理模型，deepseek，qwq等思考过程太长，一个简单的查指标问题也要分析一大段，而且中途调用工具还会反复犹豫，耗时太长且给用户展示的效果很差。

- 改进方式：使用最强的推理模型o3生成短思考+工具调用的REACT过程。但是o3有反扒机制，不会输出中间的思考过程，即使在加了think工具后，模型不能保证每次都稳定调用，这样模型一直在调用工具，但是用户不知道模型的推理过程。我们的解决方法是，在每个工具都额外添加一个参数，这个参数描述是模型想调用工具这个工具干什么。

iii. **python解释器**

- 之前的问题：取数工具是一个综合的接口，公司做的不是很好，一个问句会出来很多相关，不相关的表格（例子，xxxx），我们用字符串填充到tool call的message里面表格又多又长，需要做截断。但是取的数据（尤其是一个时间序列的）被截断之后，模型无法对结构化的数据做分析计算（举个例子，~~xx公司股价最高的三天有规律吗，~~），并且生成答案的时候，如果要画图不能画时间序列太长的图。（xx公司近5年的成长曲线是什么样的，用折线图展示公盈利能力，股票价格变化趋势）
- 改进方式：引入python代码解释器工具。1. 把取出来的数据全部转为dataframe对象，按照 sheet_1 :  df.head() + df.tail()  + df.info() 这种格式组织dataframe，模型可以看到这个dataframe前面几行，后面几行是什么，另外也知道各个shema是什么格式（字符串，float），每个shema有多少数据为空。同时在prompt中告诉模型，可以直接引用变量进行操作。2. 用ipython内核执行代码，工具返回的是stdout，traceback等关键信息。 3. 采用函数式编程方式，让模型每次生成单个功能的函数，然后打印函数执行的结果。这样可以避免模型生成一大串代码，但是代码中间其实已经报错了。（dataframe经常有空值，另外dataframe日期格式不统一，2025/7/27，20250207，2025-02-07，需要模型写代码先转换）

iv.**分层记忆管理**

亮点：三级记忆压缩。

- tool级别信息压缩。取数工具输出太长改成dataframe，显著压缩效率。搜索素材太长，把搜索工具拆分为search和fetch。search工具得到结果之后，只会返回的是 id、时间、标题、摘要。如果模型要看全文，需要使用fetch工具，传入文章id，之后可以拿到全文。对于一些无用的网页信息，模型通过标题+摘要就能筛选掉，而如果是干货比较多的，可以进一步查看细节。
- step级别信息压缩。之前的问题：搜索接口不稳定/代码执行出错，模型反复调用工具，规划思考轮数太多，导致上下文超长。（例如，模型多次修改代码得到可以正常运行的代码之后，模型在下一轮规划其实没必要带上之前的上下文，只需要保留正确的代码就行）。step级别压缩是指，如果到达70%的上下文阈值，会对中间的工具调用过程进行删除。（还可以参考openhands，不仅仅是删除，还会进行摘要。）现在还在继续验证这个方案，主要是中间有些过程不太好判断是否可以删掉。

- conversation级别压缩。之前的问题：workflow额外引入了一个问句改写功能。改进：直接把用户的历史问句+答案保存到message里面，只保留最近三轮的完整对话记录（last-3 conversation），再之前的会对答案进行摘要，（问句比较短，仍然保留），最多支持10轮对话。

  

  问1：为什么tool级别不用rag，或者embedding召回方法？

  答1：尝试过embedding方法对网页来说效果不好，有几个重大缺陷。a. 网页如果太长，需要chunk然后和搜索query做embedding匹配加rerank，效率太低，对于问答系统来说无法承受。b. 有些抽象的概念很难和正文匹配。（例如，xx公司现在值得买吗？如果把这个和正文匹配，很容易得到一些所谓自媒体发布的推广。但其实，如果保留正文中的一些新闻消息，或者公司的财务数据，模型可以自己生成观点分析，而这一类的chunk对于embedding来说很难匹配上）

#### 2.答案生成

直接把搜索到的dataframe，搜索结果，python执行记录拼在一个prompt。模型按markdown格式生成正文。如果需要画图，采用这种格式<figure> ```python 画图代码块````</figure>。画图工具采用ploty，可以生成交互的图，生成完之后执行代码运行。表格直接引用，dataframe，后台解析到替换成表格组件。

#### 3.数据合成

进行中...

### 端到端强化学习

问1：**为什么没有用端到端RL？**

答1：客观上来说，公司接口不太支持，训练集群无法调用搜索，取数接口，尤其是取数还有并发限制。

问2：如果要用端到端RL做deepresearch，要怎么设计整个训练流程，数据合成怎么做？

答2：xxx

## 其它

#### prompt怎么写？

有以下几个注意点：

- 格式化输出，最好用json或者xml。模型输出这个结构化数据比较容易，也好解析。如果格式出错可以用正则匹配。例如，模型生成工具列表： 1. report: xx 公司研报 2.search：xx新闻。这种多了需要解析的时候考虑把无关的去除。
- few-shot。只在关键的地方写，比如在智能query生成的时候，可以给几个例子帮助模型理解。举例子的时候最好格式多样一些！而不是统一的模板，例如模型的thought部分太简单，可以写几个good例子，告诉模型对于复杂问题，可以先仔细分析需要哪些信息，再做规划，而不是直接把问题扔到搜索工具中。再写一个badcase，告诉模型如果是简单的（查询今天天气，不要写一大串思考内容）。另外，格式之类的（1，（一））这种可以混用，没必要非要统一。另外生成答案的prompt，可以多准备几套，混合着用。
- 注意利用kv-cache。尽量把system做成固定的，可以变的部分，例如时间，用户问题放在后面。这样对不同的问题，可以复用kv-cache。

#### 后续改进

- 转为multi-agent方案。难点是agent之间的通信，可以类似qwen-agent分配任务，或者openhands基于时间驱动，消息订阅。最好多看看代码生成这一块，生成代码是一个比较复杂的任务，需要有很强的记忆管理：  1. 要读取代码目录，输入不会短 2. 要生成代码，cursor一次性创建多个文件，输出也长 3. 用户经常会有很复杂的需求，需要交轮次多。

- 尝试GRPO应用到答案生成，难点是评价答案的好坏，可以参考BRPO。

- 多轮对话，需要优化dataframe保存的逻辑，直接注入全部的变量太多了。
